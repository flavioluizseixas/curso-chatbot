{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.schema import Document  # Corrigir import do Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Forçar o uso da CPU, independentemente de MPS estar disponível\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar um armazenamento de documentos\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Indexar documentos (exemplo simples)\n",
    "documents = [\n",
    "    {\"content\": \"A linguagem Python é popular para machine learning.\"},\n",
    "    {\"content\": \"O framework Haystack é usado para criar pipelines de NLP.\"},\n",
    "    {\"content\": \"GPT é um modelo de linguagem desenvolvido pela OpenAI.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona documentos ao armazenamento\n",
    "document_store.write_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fseixas/anaconda3/envs/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Configurar um recuperador DPR (Dense Passage Retriever)\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [00:00, 21201.84 docs/s]     \n"
     ]
    }
   ],
   "source": [
    "# 4. Atualizar o armazenamento de documentos com embeddings do retriever\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Criar um pipeline de perguntas e respostas\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\")\n",
    "qa_pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo e o tokenizer do GPT-2 (ou GPT-Neo para um modelo maior)\n",
    "model_name = \"gpt2\"  # ou \"EleutherAI/gpt-neo-125M\" para GPT-Neo\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para geração de texto usando GPT-2 local\n",
    "def generate_response(context, user_query, max_length=100):\n",
    "    prompt = f\"User question: {user_query}\\n\\nContext: {context}\\n\\nAI response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    # Geração de texto usando o modelo local\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            max_length=max_length, \n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para recuperação de documentos\n",
    "def retrieve_documents(query):\n",
    "    results = qa_pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 3}, \"Reader\": {\"top_k\": 1}})\n",
    "    return results['answers'][0].answer\n",
    "\n",
    "# Função para recuperação de documentos\n",
    "#def retrieve_documents(query):\n",
    "#    results = qa_pipeline.run(query=query, top_k_retriever=3, top_k_reader=1)\n",
    "#    return results['answers'][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal do chatbot usando RAG com LLM local\n",
    "def chatbot(user_query):\n",
    "    # Passo 1: Recuperar contexto relevante\n",
    "    retrieved_context = retrieve_documents(user_query)\n",
    "    \n",
    "    # Passo 2: Gerar resposta com base no contexto recuperado\n",
    "    response = generate_response(retrieved_context, user_query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples:   0%|          | 0/1 [00:00<?, ? Batches/s]/Users/fseixas/anaconda3/envs/myenv/lib/python3.11/site-packages/haystack/modeling/model/prediction_head.py:471: UserWarning: The operator 'aten::tril_indices' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  indices = torch.tril_indices(max_seq_len, max_seq_len, offset=-1, device=start_end_matrix.device)\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.29 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: User question: oi\n",
      "\n",
      "Context: Python é popular para machine learning.\n",
      "\n",
      "AI response:\n",
      "\n",
      "The following is a list of questions that I have been asked by people who have been asked this question.\n",
      "\n",
      "Question 1: What is the most important thing you want to learn about AI?\n",
      "\n",
      "Answer: I want to learn about AI.\n",
      "\n",
      "Question 2: What is the most important thing you want to learn about AI?\n",
      "\n",
      "Answer: I want to learn about\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  3.98 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: User question: qual linguagem é mais popular?\n",
      "\n",
      "Context: Python\n",
      "\n",
      "AI response:\n",
      "\n",
      "I'm not sure if you can say that I'm a linguagem é mais popular. I'm not sure if you can say that I'm a linguagem é mais popular.\n",
      "\n",
      "I'm not sure if you can say that I'm a linguagem é mais popular.\n",
      "\n",
      "I'm not sure if you can say that I'm a\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de interação com o chatbot\n",
    "while True:\n",
    "    user_input = input(\"Pergunta: \")\n",
    "    if user_input.lower() in ['sair', 'exit']:\n",
    "        break\n",
    "    answer = chatbot(user_input)\n",
    "    print(f\"Chatbot: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
