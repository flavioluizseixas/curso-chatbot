{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.schema import Document  # Corrigir import do Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Forçar o uso da CPU, independentemente de MPS estar disponível\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Criar um armazenamento de documentos\n",
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Indexar documentos (exemplo simples)\n",
    "documents = [\n",
    "    {\"content\": \"A linguagem Python é popular para machine learning.\"},\n",
    "    {\"content\": \"O framework Haystack é usado para criar pipelines de NLP.\"},\n",
    "    {\"content\": \"GPT é um modelo de linguagem desenvolvido pela OpenAI.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona documentos ao armazenamento\n",
    "document_store.write_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flavio/Dev/myenv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Configurar um recuperador DPR (Dense Passage Retriever)\n",
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [00:00, 16318.17 docs/s]     \n"
     ]
    }
   ],
   "source": [
    "# 4. Atualizar o armazenamento de documentos com embeddings do retriever\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Criar um pipeline de perguntas e respostas\n",
    "reader = FARMReader(model_name_or_path=\"pierreguillou/bert-large-cased-squad-v1.1-portuguese\")\n",
    "qa_pipeline = ExtractiveQAPipeline(reader=reader, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo e o tokenizer do GPT-2 (ou GPT-Neo para um modelo maior)\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"  # Um modelo adequado para várias línguas, incluindo português\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPTNeoForCausalLM.from_pretrained(model_name).to(torch.device(device))  # Forçando uso da CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar saudações e outras respostas simples\n",
    "def predefined_responses(user_input):\n",
    "    respostas = {\n",
    "        \"oi\": \"Oi, tudo bem?\",\n",
    "        \"olá\": \"Olá! Como posso te ajudar?\",\n",
    "        \"bom dia\": \"Bom dia! Como você está?\",\n",
    "        \"boa tarde\": \"Boa tarde! Em que posso ajudar?\",\n",
    "        \"boa noite\": \"Boa noite! O que você gostaria de saber?\"\n",
    "    }\n",
    "    \n",
    "    # Verificar se a pergunta do usuário está nas saudações predefinidas\n",
    "    user_input_lower = user_input.lower()\n",
    "    if user_input_lower in respostas:\n",
    "        return respostas[user_input_lower]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para geração de texto usando GPT-2 local\n",
    "def generate_response(context, user_query, max_length=100):\n",
    "    # Verificar saudações simples\n",
    "    predefined = predefined_responses(user_query)\n",
    "    if predefined:\n",
    "        return predefined\n",
    "    \n",
    "    prompt = f\"Pergunta do usuário: {user_query}\\n\\nContexto: {context}\\n\\nResposta da IA:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    # Reduzir o comprimento máximo\n",
    "    max_length = 100  # Definir um limite razoável para a resposta\n",
    "\n",
    "    # Geração de texto usando o modelo local\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=max_length,\n",
    "            repetition_penalty=1.5,  # Penalidade para evitar repetições\n",
    "            no_repeat_ngram_size=2,  # Evita repetições de bigramas (pares de palavras)\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para recuperação de documentos\n",
    "def retrieve_documents(query):\n",
    "    results = qa_pipeline.run(query=query, params={\"Retriever\": {\"top_k\": 3}, \"Reader\": {\"top_k\": 1}})\n",
    "    return results['answers'][0].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função principal do chatbot usando RAG com LLM local\n",
    "def chatbot(user_query):\n",
    "    # Passo 1: Recuperar contexto relevante\n",
    "    retrieved_context = retrieve_documents(user_query)\n",
    "    \n",
    "    # Passo 2: Gerar resposta com base no contexto recuperado\n",
    "    response = generate_response(retrieved_context, user_query)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:01<00:00,  1.87s/ Batches]\n",
      "/home/flavio/Dev/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/flavio/Dev/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Pergunta do usuário: qual principal linguagem utilizada para machine learning?\n",
      "\n",
      "Contexto: Python\n",
      "\n",
      "Resposta da IA:\n",
      "1. A maioria dos computadores devem ser\n"
     ]
    }
   ],
   "source": [
    "# Exemplo de interação com o chatbot\n",
    "while True:\n",
    "    user_input = input(\"Pergunta: \")\n",
    "    if user_input.lower() in ['sair', 'exit']:\n",
    "        break\n",
    "    answer = chatbot(user_input)\n",
    "    print(f\"Chatbot: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
